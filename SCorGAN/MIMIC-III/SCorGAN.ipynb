{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZKq_k35W6_a"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.pyplot import figure\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "or158iZXcTtF",
    "outputId": "0e2c79c1-7878-4a73-e64b-d35fd7a74435",
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "experimentName = 'SCorGAN.MIMIC-III'\n",
    "\n",
    "parser.add_argument(\"--datasetpath\", type=str, default=os.path.expanduser('~/workspace/data/mimic-iii-processed/BINARY.h5'), help=\"Dataset file\")\n",
    "\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=100, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--n-epochs-ae\", type=int, default=100, help=\"number of epochs of autoencoder training\")\n",
    "parser.add_argument(\"--batch-size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.001, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--n-cpu\", type=int, default=32, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument('--n-critic', type=int, default=5, help='number of Discriminator iterations per each Generator iteration')\n",
    "parser.add_argument('--clamp', type=float, default=0.01, help='weight clipping value')\n",
    "parser.add_argument(\"--cuda\", type=bool, default=True, help=\"CUDA activation\")\n",
    "parser.add_argument(\"--latent-dim\", type=int, default=128, help=\"dimensionality of the latent space\")\n",
    "\n",
    "parser.add_argument(\"--pretrained\", type=bool, default=False, help=\"Use pretrained model\")\n",
    "parser.add_argument(\"--pretrained-ae\", type=bool, default=False, help=\"Use pretrained model for autoencoder\")\n",
    "\n",
    "parser.add_argument(\"--expPATH\", type=str, default=os.path.expanduser('~/workspace/pytorch-exports/models/{}'.format(experimentName)), help=\"Export Path\")\n",
    "\n",
    "opt = parser.parse_args([])\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3syZZGE3ixcZ",
    "outputId": "895eee16-5937-433a-a7a0-120bb0a5eb17",
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################\n",
    "### Initialization ###\n",
    "######################\n",
    "\n",
    "# Create experiments DIR\n",
    "if not os.path.exists(opt.expPATH):\n",
    "    os.system('mkdir -p {0}'.format(opt.expPATH))\n",
    "\n",
    "# opt.seed = 1024 # fix seed\n",
    "opt.seed = random.randint(1, 10000)\n",
    "\n",
    "print('Random Seed: {}'.format(opt.seed))\n",
    "random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "np.random.seed(opt.seed)\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device BUT it is not in use...\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "print('using \"{}\" as the tensor processor'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYJtIeTEil-P"
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "### Reading Dataset from File ###\n",
    "#################################\n",
    "\n",
    "input_data = None\n",
    "with h5py.File(opt.datasetpath, 'r') as hf:\n",
    "    input_data = hf.get('dataset')[()]\n",
    "\n",
    "sample_size = input_data.shape[0]\n",
    "feature_size = input_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP96S8Kbcs0y"
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "### Dataset Model ###\n",
    "#####################\n",
    "\n",
    "class EHRDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.sample_size = dataset.shape[0]\n",
    "        self.feature_size = dataset.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXkAxe6HS-KJ",
    "outputId": "9fc9179b-46f2-4ffd-db3f-d04d2bc7d376",
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################8, 43\n",
    "### Dataset Processing ###\n",
    "##########################\n",
    "\n",
    "train_data = input_data[:int(0.8 * sample_size)]\n",
    "test_data = input_data[int(0.8 * sample_size):]\n",
    "print('total samples: {}, features: {}'.format(sample_size, feature_size))\n",
    "print('training data shape: {}, testing data shape: {}, dataset type: {}'.format(train_data.shape, test_data.shape, input_data.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvS8xM1k8Teu"
   },
   "outputs": [],
   "source": [
    "training_dataloader = DataLoader(\n",
    "    EHRDataset(dataset=train_data),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu\n",
    ")\n",
    "\n",
    "testing_dataloader = DataLoader(\n",
    "    EHRDataset(dataset=test_data),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jSYLExToytb"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "### AutoEncoder Loss ###\n",
    "########################\n",
    "\n",
    "class AutoEncoderLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoderLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        epsilon = 1e-12\n",
    "        term = target * torch.log(input + epsilon) + (1. - target) * torch.log(1. - input + epsilon)\n",
    "        return -torch.sum(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch3VtsBwn5p9"
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "### AutoEncoder Model ###\n",
    "#########################\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        # input is batch_size * 1071\n",
    "        # output is batch_size * 16 * 131\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=8, kernel_size=11, stride=4, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv1d(in_channels=8, out_channels=16, kernel_size=7, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.encoder_linear = nn.Sequential(\n",
    "            nn.Linear(16 * 131, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, opt.latent_dim),\n",
    "            nn.BatchNorm1d(opt.latent_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder_linear = nn.Sequential(\n",
    "            nn.Linear(opt.latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 16 * 131),\n",
    "            nn.BatchNorm1d(16 * 131),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=7, stride=2, padding=0),\n",
    "            nn.BatchNorm1d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose1d(in_channels=8, out_channels=1, kernel_size=11, stride=4, padding=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = x.view(-1, 1, feature_size)\n",
    "        x = self.encoder(x).view(x.shape[0], -1)\n",
    "        return self.encoder_linear(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = self.decoder_linear(x)\n",
    "        x = x.view(x.shape[0], 16, 131)\n",
    "        return torch.squeeze(self.decoder(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgp1ytGXFDwS"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "### Model Initialization ###\n",
    "############################\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "autoencoder = Autoencoder().cuda() if opt.cuda else Autoencoder()\n",
    "optimizer_A = torch.optim.Adam(autoencoder.parameters(), lr=opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jWlJt7_892vs",
    "outputId": "56d494cb-e816-4cb6-db80-441048dc4a61",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "###### AutoEncoder Training #########\n",
    "#####################################\n",
    "\n",
    "criterion = AutoEncoderLoss()\n",
    "\n",
    "if not opt.pretrained_ae:\n",
    "    for epoch in range(opt.n_epochs_ae):\n",
    "        train_loss = 0\n",
    "        autoencoder.train()\n",
    "        for batch in training_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            gen_batch = autoencoder(batch)\n",
    "            loss_A = criterion(gen_batch, batch)\n",
    "            optimizer_A.zero_grad()\n",
    "            loss_A.backward()\n",
    "            optimizer_A.step()\n",
    "            train_loss += loss_A\n",
    "\n",
    "        with torch.no_grad():\n",
    "            errors = 0\n",
    "            test_loss = 0\n",
    "            autoencoder.eval()\n",
    "            for batch in testing_dataloader:\n",
    "                batch = batch.to(device)\n",
    "                gen_batch = autoencoder(batch)\n",
    "                test_loss += criterion(gen_batch, batch)\n",
    "\n",
    "                gen_batch = gen_batch.round()\n",
    "                diff = torch.abs(gen_batch - batch).view(-1).detach().cpu().numpy()\n",
    "                wrong_digits = diff[diff > 0.5]\n",
    "                errors += len(wrong_digits)\n",
    "            print(\"[Epoch {:3d}/{:3d}] [Training Loss: {:12.2f}] [Testing Loss: {:10.2f}] [errors: {:6d}]\".format(epoch + 1, opt.n_epochs_ae, train_loss, test_loss, errors), flush=True)\n",
    "    torch.save(autoencoder.state_dict(), opt.expPATH + '/autoencoder.model')\n",
    "else:\n",
    "    autoencoder.load_state_dict(torch.load(opt.expPATH + '/autoencoder.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc5-uQh2AOUr"
   },
   "outputs": [],
   "source": [
    "def mbss(x):\n",
    "    x = F.pad(input=x, pad=(0, 9), mode='constant', value=0)\n",
    "    x = x.view(x.shape[0], -1, 10)\n",
    "    x = torch.mean(x, 2)\n",
    "    return torch.mean(x, 0).repeat(x.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0I5dbr5cGlQ"
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "### Generator Model ###\n",
    "#############################\n",
    "\n",
    "# Output should be batch_size * opt.latent_dim\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose1d(opt.latent_dim, 64, 4, 2, 1),\n",
    "            nn.BatchNorm1d(64, eps=0.0001, momentum=0.01),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm1d(32, eps=0.0001, momentum=0.01),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.ConvTranspose1d(32, 16, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.view(-1, opt.latent_dim, 1)).view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5oIATq7r9ek"
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### Discriminator Model ###\n",
    "###########################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        ndf = 16\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # minibatch subsampling, new feature size is 1071 + 108\n",
    "            nn.Linear(1179, feature_size),\n",
    "\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv1d(1, ndf, 8, 4, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv1d(ndf, ndf * 2, 8, 4, 1),\n",
    "            nn.BatchNorm1d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv1d(ndf * 2, ndf * 4, 8, 4, 1),\n",
    "            nn.BatchNorm1d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv1d(ndf * 4, ndf * 8, 8, 4, 1),\n",
    "            nn.BatchNorm1d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv1d(ndf * 8, 1, 3, 1, 0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # minibatch subsampling\n",
    "        x = torch.cat((x, mbss(x)), 1)\n",
    "        return self.model(x.view(x.shape[0], 1, -1)).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7inVoiMu9QS_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "############################\n",
    "### Model Initialization ###\n",
    "############################\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "generator = Generator().cuda() if opt.cuda else Generator()\n",
    "discriminator = Discriminator().cuda() if opt.cuda else Discriminator()\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e83EKUAC3Edo",
    "outputId": "a0f35388-2a64-4871-eaa6-5eeb2f845f5a",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if not opt.pretrained:\n",
    "    batches_done = 0\n",
    "\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "\n",
    "    for epoch in range(opt.n_epochs):\n",
    "        epoch_start = time.time()\n",
    "        for _ in range(opt.n_critic):\n",
    "            for batch in training_dataloader:\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                batch = batch.to(device)\n",
    "\n",
    "                for dp in discriminator.parameters():\n",
    "                    dp.requires_grad = True\n",
    "\n",
    "                optimizer_D.zero_grad()\n",
    "\n",
    "                z = torch.randn(batch.shape[0], opt.latent_dim, device=device)\n",
    "                fake_batch = autoencoder.decode(generator(z))\n",
    "                loss_D = torch.mean(discriminator(batch), dim=0) - torch.mean(discriminator(fake_batch.detach()), dim=0)\n",
    "                loss_D.backward()\n",
    "\n",
    "                optimizer_D.step()\n",
    "\n",
    "                for dp in discriminator.parameters():\n",
    "                    dp.data.clamp_(-opt.clamp, opt.clamp)\n",
    "\n",
    "                if batches_done % opt.n_critic == 0:\n",
    "                    # -----------------\n",
    "                    #  Train Generator\n",
    "                    # -----------------\n",
    "                    for dp in discriminator.parameters():\n",
    "                        dp.requires_grad = False\n",
    "\n",
    "                    optimizer_G.zero_grad()\n",
    "\n",
    "                    z = torch.randn(batch.shape[0], opt.latent_dim, device=device)\n",
    "                    fake_batch = autoencoder.decode(generator(z))\n",
    "                    loss_G = torch.mean(discriminator(fake_batch), dim=0)\n",
    "                    loss_G.backward()\n",
    "\n",
    "                    optimizer_G.step()\n",
    "\n",
    "                batches_done += 1\n",
    "                if batches_done % (100 * opt.n_critic) == 0:\n",
    "                    print('[Epoch {:3d}/{:3d}] [Batch {:4d}/{:4d}] [D loss: {:.6f}] [G loss: {:.6f}]'.format(epoch + 1, opt.n_epochs, batches_done % (opt.n_critic * len(training_dataloader)), opt.n_critic * len(training_dataloader), loss_D.item(), loss_G.item()))\n",
    "\n",
    "        print('[Epoch {:3d}/{:3d}] [Time: {:.2f}] [D loss: {:.6f}] [G loss: {:.6f}]'.format(epoch + 1, opt.n_epochs, time.time() - epoch_start, loss_D.item(), loss_G.item()))\n",
    "\n",
    "    torch.save(generator.state_dict(), opt.expPATH + '/generator.model')\n",
    "    torch.save(discriminator.state_dict(), opt.expPATH + '/discriminator.model')\n",
    "else:\n",
    "    generator.load_state_dict(torch.load(opt.expPATH + '/generator.model'))\n",
    "    discriminator.load_state_dict(torch.load(opt.expPATH + '/discriminator.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEcQ_acg1xqu",
    "outputId": "556aba9a-d5f0-41d1-cee8-b8227091948e"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "generator.eval()\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuCmebS-zPp5"
   },
   "outputs": [],
   "source": [
    "num_fake_batches = 80\n",
    "fake_data = torch.zeros((0, feature_size), device='cpu')\n",
    "for _ in range(num_fake_batches):\n",
    "  z = torch.randn(opt.batch_size, 128, device=device)\n",
    "  generated_batch = generator(z)\n",
    "  fake_batch = autoencoder.decode(generator(z))\n",
    "  fake_data = torch.cat((fake_data, fake_batch.round().to('cpu')), 0)\n",
    "np.save(os.path.join(opt.expPATH, \"synthetic.npy\"), fake_data.detach().cpu().numpy(), allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YqZaV7D_EXi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SCorGAN.MBSS.ETD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
